# ğŸ—ï¸ Enterprise RAG Platform

### ğŸš€ Intelligent Context-Aware Knowledge Retrieval for Enterprises

---

## ğŸ“˜ Overview

**Enterprise RAG Platform** is a scalable, production-grade system designed to deliver **contextual, accurate, and explainable answers** over massive enterprise document collections (1M+ docs) with **sub-second latency**.

It follows a **modular microservices architecture** combining:

- **Hybrid Retrieval** (Dense + BM25),
- **Semantic Caching**, and
- **LLM-Orchestrated Generation**

The platform is built with **LangChain-style orchestration**, **Redis semantic cache**, **Vector Databases** (Milvus/Weaviate), and **LLMs** served via **Triton/vLLM** â€” all deployed on **Kubernetes** for auto-scaling and observability.

---

## ğŸ¯ End Goal (Full Platform Vision)

> Build a _Google Searchâ€“level Enterprise AI Assistant_ that can:
>
> - Ingest millions of internal documents (policies, tickets, manuals)
> - Answer user questions with **grounded, source-cited, trustworthy** responses
> - Learn continuously from user feedback
> - Scale horizontally while maintaining <1 second response time
> - Operate cost-effectively and securely in multi-tenant enterprise setups

---

### ğŸ§© Architectural Summary

```
User â†’ API Gateway â†’ Auth â†’ RAG Orchestrator
      â†³ Semantic Cache (Redis + FAISS)
      â†³ Retriever (Dense + BM25)
      â†³ Reranker (Cross-Encoder)
      â†³ Vector DB (Milvus/Weaviate)
      â†³ LLM Gateway (vLLM / Triton)
      â†³ Monitoring (Prometheus + Grafana)
      â†³ Feedback Loop (Kafka)
```

The orchestrator acts as the **brain** that coordinates retrieval, ranking, caching, generation, and feedback.

---

## ğŸ§± Phase Roadmap (Iterative Build Plan)

We are building the platform in **5 phases**, each producing a working increment of the full system.

| Phase       | Focus                          | Goal                                  | Expected Output                            |
| ----------- | ------------------------------ | ------------------------------------- | ------------------------------------------ |
| **Phase 1** | MVP Core Orchestrator          | Functional end-to-end mock pipeline   | `/v1/query` API returning mock answer      |
| **Phase 2** | Real Retrieval Layer           | Hybrid dense + sparse retrievers      | Real context retrieval from docs           |
| **Phase 3** | LLM Integration                | Real model inference (local or API)   | Factual generation with citations          |
| **Phase 4** | Feedback Loop & Metrics        | RAG evaluation + learning             | Continuous improvement via feedback        |
| **Phase 5** | Kubernetes & Productionization | Full infra + monitoring + autoscaling | Deployed, observable, self-healing cluster |

---

## ğŸ§  Phase 1 â€” â€œMocked Brainâ€ MVP (Current Phase)

### ğŸ¯ Goal

To build a **working prototype** of the entire RAG pipeline using **mock components** that simulate retrieval, ranking, and generation â€” proving that the architecture and API contract are correct.

This establishes the **foundation** of all future phases.

---

### ğŸ§© Scope of Phase 1

| Component                      | Description                                       |
| ------------------------------ | ------------------------------------------------- |
| **RAG Orchestrator (FastAPI)** | Central brain coordinating the pipeline           |
| **Semantic Cache (Redis)**     | Store recent query responses (by query hash)      |
| **Mock Retriever**             | Returns fake but structured text chunks           |
| **Mock Reranker**              | Sorts mock chunks randomly or by score            |
| **Mock LLM Generator**         | Returns templated answers based on fake context   |
| **Logging + Metrics**          | Print latency and cache hits to logs              |
| **Unit Tests (pytest)**        | Validate orchestrator, caching, and output schema |
| **Local Deployment**           | Run via Docker Compose (no Kubernetes yet)        |

---

### ğŸ§  Example Flow (Phase 1)

1. User sends a query â†’ `/v1/query`
2. Orchestrator checks Redis for cached answer.
3. If miss â†’ calls mock retriever â†’ mock reranker â†’ mock LLM.
4. Returns structured JSON answer with fake citations.
5. Logs latency and stores result in cache.

**Example Output:**

```json
{
  "answer": "Refunds are processed within 7 days of purchase (mock).",
  "sources": [
    {
      "doc_id": "mock_doc_001",
      "snippet": "Refunds within 7 days...",
      "score": 0.91
    }
  ],
  "meta": { "cache_hit": false, "latency_ms": 420 }
}
```

---

### ğŸ§± Phase 1 Architecture Diagram

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   User / Client     â”‚
                 â”‚ (curl / Postman)    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚  POST /v1/query
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   RAG Orchestrator     â”‚
                 â”‚  (FastAPI Service)     â”‚
                 â”‚------------------------â”‚
                 â”‚ - Input validation     â”‚
                 â”‚ - Cache check          â”‚
                 â”‚ - Mock retrieval       â”‚
                 â”‚ - Mock reranking       â”‚
                 â”‚ - Mock generation      â”‚
                 â”‚ - Response assembly    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚              â”‚                â”‚
             â–¼              â–¼                â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Redis Cacheâ”‚ â”‚ MockRetrieverâ”‚ â”‚ Mock LLM    â”‚
      â”‚ (Semantic) â”‚ â”‚ (JSON corpus)â”‚ â”‚ (Template)  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Console Logâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ§© Phase 1 Deliverables

- âœ… `/v1/query` REST endpoint
- âœ… Structured JSON responses
- âœ… Redis cache integration
- âœ… Basic logging & latency metrics
- âœ… Unit tests (pytest) passing
- âœ… Docker Compose setup (`orchestrator + redis`)
- âœ… README & design docs

---

### ğŸ§  Success Criteria for Phase 1

| Category      | Metric / Goal                                 |
| ------------- | --------------------------------------------- |
| API Response  | < 1 second average latency                    |
| Cache Hit     | Confirmed on repeated queries                 |
| Test Coverage | â‰¥ 80% orchestrator coverage                   |
| Stability     | Containers restart cleanly via Docker Compose |
| Observability | Logs show latency, cache hits                 |

---

## ğŸ§± Brief Preview of Next Phases

### **Phase 2 â€” Real Retrieval Layer**

- Replace MockRetriever with hybrid retrieval:

  - BM25 (ElasticSearch or Postgres full-text)
  - Dense retrieval (FAISS or Milvus)

- Add score normalization & reranker model
- Measure recall/precision on test corpus

### **Phase 3 â€” LLM Integration**

- Integrate real LLM via vLLM, Triton, or OpenAI API
- Add prompt templates, context packing, token budgeting
- Implement fallback & retry logic
- Measure factual accuracy and latency

### **Phase 4 â€” Feedback & Evaluation**

- Add Kafka-based feedback pipeline
- Integrate RAGAS evaluation framework
- Build feedback dashboard (Grafana + API)
- Start continuous improvement loop

### **Phase 5 â€” Kubernetes & Productionization**

- Deploy all services (RAG orchestrator, retrievers, LLM, cache)
- Add observability (Prometheus, Loki, Jaeger)
- Enable autoscaling (HPA)
- Implement canary rollout (Argo Rollouts)
- Meet SLOs: p95 latency < 1s, uptime > 99.9%

---

## ğŸ§° Tech Stack Summary

| Layer                 | Technology                            |
| --------------------- | ------------------------------------- |
| **Backend Framework** | FastAPI (Python 3.11)                 |
| **Cache**             | Redis                                 |
| **Database**          | PostgreSQL (Phase 2+)                 |
| **Vector DB**         | Milvus / FAISS (Phase 2+)             |
| **LLM Serving**       | vLLM / Triton / OpenAI API (Phase 3+) |
| **Queue / Feedback**  | Kafka (Phase 4)                       |
| **Infra & Deploy**    | Docker Compose â†’ Kubernetes (Phase 5) |
| **Monitoring**        | Prometheus, Grafana, Loki (Phase 5)   |
| **CI/CD**             | GitHub Actions + ArgoCD               |
| **Testing**           | pytest (TDD), behave/pytest-bdd (BDD) |

---

## ğŸ§­ Development Approach

- **Test-Driven Development (TDD)** for service logic
- **Behavior-Driven Development (BDD)** for user flows
- **Infrastructure-as-Code (IaC)** via Terraform + Helm (Phase 5)
- **GitOps Deployment** using ArgoCD (Phase 5)

---

## ğŸ’¡ Why Build Incrementally?

Because each phase is a **complete working slice** that can:

- Be tested, deployed, and benchmarked independently
- Serve as a learning checkpoint
- Prevent wasted work on premature optimization

By Phase 2, youâ€™ll have a functioning RAG backend.
By Phase 3, a real AI system with context-based answers.
By Phase 5, a fully scalable, observable production system.

---

## ğŸ“ˆ Final Vision Diagram (Phase 5 Reference)

```
 End User â†’ API Gateway â†’ Auth â†’ RAG Orchestrator
               â†“           â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        â”‚ SemanticCacheâ”‚   â”‚
        â”‚ (Redis/FAISS)â”‚   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
               â†“            â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Retriever     â”‚ â”‚ Vector DB     â”‚
        â”‚ (BM25 + ANN)  â”‚ â”‚ (Milvus/FAISS)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Reranker     â”‚
        â”‚ (CrossEnc.)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ LLM Inferenceâ”‚
        â”‚ (vLLM/Triton)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Feedback Loopâ”‚
        â”‚ (Kafka + Eval)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Monitoring   â”‚
        â”‚ (Prom/Grafana)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ Summary

- Weâ€™re currently building **Phase 1: the Mock Orchestrator MVP.**
- This gives us a **working skeleton** of the system: `/v1/query` â†’ mock retrieval â†’ mock LLM â†’ cached output.
- Next, weâ€™ll plug in **real retrievers and LLMs** while preserving this tested foundation.
